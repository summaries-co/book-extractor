{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6bObSuDlfnI"
      },
      "source": [
        "This notebook generates **Key Takeaways** summary for each chapter of a book using OpenAI's API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tO3kdpYQlUua",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Enter the required information for generating Key Takeaways.\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown **Step 1:** Enter API Key and ISBN\n",
        "\n",
        "open_api_key = '' #@param {type:\"string\"}\n",
        "isbn = '' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTPWB-BkmDUC",
        "outputId": "8e9f9ab5-9b27-4c78-8d80-349f50112727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.23.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install transformers\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from transformers import GPT2TokenizerFast\n",
        "from openai import OpenAI\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import concurrent.futures\n",
        "import io\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown **Step 2:** Upload Chapter Extracts File\n",
        "\n",
        "#@markdown Click on the **\"Choose Files\"** button to upload your chapter extracts file.\n",
        "\n",
        "#@markdown The file should be in JSON format with the following structure for each chapter:\n",
        "\n",
        "#@markdown ```\n",
        "#@markdown {\n",
        "#@markdown   \"isbn\": \"ISBN-10\",\n",
        "#@markdown   \"name\": \"Chapter Name\",\n",
        "#@markdown   \"sequence_index\": index,\n",
        "#@markdown   \"contents\": \"Chapter text\",\n",
        "#@markdown   \"part\": \"Chapter part number\",\n",
        "#@markdown }\n",
        "#@markdown ```\n",
        "#@markdown Once the file is selected, it will be automatically uploaded.\n",
        "\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "if not uploaded_files:\n",
        "    raise Exception(\"File upload failed, please try again.\")\n",
        "else:\n",
        "    # Taking only the first uploaded file\n",
        "    uploaded_filename = next(iter(uploaded_files))\n",
        "    uploaded_file = uploaded_files[uploaded_filename]\n",
        "\n",
        "    # Reading the file\n",
        "try:\n",
        "    data = json.load(io.BytesIO(uploaded_file))\n",
        "    # Check that all necessary keys are present\n",
        "    if not all(key in chapter for chapter in data for key in [\"isbn\", \"name\", \"sequence_index\", \"contents\", \"part\"]):\n",
        "        raise ValueError(\"Some chapters do not contain all required keys.\")\n",
        "    print(f\"File '{uploaded_filename}' successfully uploaded and read.\")\n",
        "except (json.JSONDecodeError, ValueError) as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "FkkyodgMok9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "9011d14c-4a6a-418a-9c7c-d8700f1cc012"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0923064a-fa29-4314-9b97-7521fc9c629e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0923064a-fa29-4314-9b97-7521fc9c629e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1626813582_autosplits.json to 1626813582_autosplits (1).json\n",
            "File '1626813582_autosplits (1).json' successfully uploaded and read.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6gwGhsaamLka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe66ef64-e991-4685-fab6-5749c29f1cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompting model...\n",
            "Prompting model...\n",
            "Prompting model...Prompting model...\n",
            "Prompting model...\n",
            "Prompting model...\n",
            "\n",
            "Prompting model...Prompting model...\n",
            "\n",
            "Response received -- Time taken: 6.68 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.68 seconds\n",
            "Response received -- Time taken: 7.79 seconds\n",
            "Prompting model...Response received -- Time taken: 7.82 seconds\n",
            "\n",
            "Prompting model...\n",
            "Prompting model...\n",
            "Response received -- Time taken: 9.33 seconds\n",
            "Response received -- Time taken: 9.55 seconds\n",
            "Prompting model...Response received -- Time taken: 9.65 seconds\n",
            "\n",
            "Prompting model...\n",
            "Prompting model...\n",
            "Response received -- Time taken: 11.97 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 4.68 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 5.77 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 6.35 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 6.12 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 9.25 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.00 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.21 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 8.45 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.22 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 5.63 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 9.26 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 11.96 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 8.64 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 10.01 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 6.25 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 6.02 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 8.83 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 9.57 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.69 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 10.02 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 6.19 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 20.12 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.31 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 9.58 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.47 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.09 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 8.53 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.86 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 16.50 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 6.60 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 6.68 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 9.26 seconds\n",
            "Response received -- Time taken: 8.32 seconds\n",
            "Response received -- Time taken: 12.55 seconds\n",
            "Response received -- Time taken: 8.34 secondsPrompting model...\n",
            "\n",
            "Prompting model...\n",
            "Prompting model...\n",
            "Prompting model...\n",
            "Response received -- Time taken: 9.75 seconds\n",
            "Response received -- Time taken: 7.38 seconds\n",
            "Prompting model...Prompting model...\n",
            "\n",
            "Response received -- Time taken: 10.35 seconds\n",
            "Response received -- Time taken: 6.71 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.60 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.30 seconds\n",
            "Response received -- Time taken: 8.38 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 10.63 seconds\n",
            "Response received -- Time taken: 9.96 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 10.67 seconds\n",
            "Prompting model...\n",
            "Response received -- Time taken: 7.68 seconds\n",
            "Response received -- Time taken: 7.25 seconds\n",
            "Response received -- Time taken: 6.45 seconds\n",
            "Response received -- Time taken: 9.41 seconds\n",
            "Response received -- Time taken: 9.54 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2208 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1849 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Foreword has 2208 tokens.\n",
            "Chapter Exponential Organizations has 1849 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4582 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (6352 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Chapter One: Illuminated by Information has 4582 tokens.\n",
            "Chapter Chapter Two: A Tale of Two Companies has 6352 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2138 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Chapter Three: The Exponential Organization has 513 tokens.\n",
            "Chapter Massive Transformative Purpose (MTP) has 2138 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1808 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1687 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Staff on Demand has 1808 tokens.\n",
            "Chapter Community & Crowd has 1687 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2469 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3653 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Algorithms has 2469 tokens.\n",
            "Chapter Engagement has 3653 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1995 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Dashboards has 1995 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3068 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Experimentation has 3068 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2408 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Autonomy has 2408 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3059 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Social Technologies has 3059 tokens.\n",
            "Chapter Chapter Five: Implications of Exponential Organizations has 400 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1700 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 1. Information Accelerates Everything has 1700 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1905 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 6. Smaller Beats Bigger has 1905 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1712 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 9. Everything is Measurable and Anything is Knowable has 1712 tokens.\n",
            "Chapter Chapter Six: Starting an ExO has 861 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1544 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Ignition has 1544 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1691 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Step 4: Breakthrough Idea has 1691 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Example 2: GitHub has 2085 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2356 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1936 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Example 5: GoPro has 2356 tokens.\n",
            "Chapter Chapter Eight: ExOs for Large Organizations has 1936 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1621 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1654 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 2. Partner with, Invest in or Acquire ExOs has 1621 tokens.\n",
            "Chapter Inspire ExOs at the Edge has 1654 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1850 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2558 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Partner with Accelerators, Incubators and Hackerspaces has 1850 tokens.\n",
            "Chapter Chapter Ten: The Exponential Executive has 2558 tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4066 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter Epilogue: A New Cambrian Explosion has 4066 tokens.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c19c76a6-6a97-439f-a7fa-04616c49835e\", \"1626813582_key_takeaways.json\", 328435)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown **Step 3:** Generate Key Takeaways and Download Processed Data\n",
        "\n",
        "#@markdown Click the **\"Play\"** button on the left of this cell to generate the key takeaways and download the processed data.\n",
        "\n",
        "# Gets the token count using GPT-2 tokenizer\n",
        "def get_token_count(text):\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "    tokens = tokenizer(text)[\"input_ids\"]\n",
        "    return len(tokens)\n",
        "\n",
        "def prompt_model(text, model=\"gpt-3.5-turbo\", open_api_key=open_api_key):\n",
        "    try:\n",
        "        start = time.time()\n",
        "        client = OpenAI(api_key=open_api_key)\n",
        "        print(\"Prompting model...\")\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": text}]\n",
        "        )\n",
        "        response = chat_completion.choices[0].message.content\n",
        "        print(\"Response received -- Time taken: {:.2f} seconds\".format(time.time() - start))\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to prompt model: {e}\")\n",
        "        raise\n",
        "\n",
        "def prompt_response_to_pov(response):\n",
        "    response.lower()\n",
        "    if \"first\" in response:\n",
        "        return \"first person\"\n",
        "    elif \"second\" in response:\n",
        "        return \"second person\"\n",
        "    else:\n",
        "        return \"third person\"\n",
        "\n",
        "def determine_pov(text):\n",
        "    model = 'gpt-4-1106-preview'\n",
        "    prompt = f'''\n",
        "    You are a editor. Determine the point of view of the text enclosed in backticks (```)\n",
        "    Respond with one of the following: first person, second person, or third person.\n",
        "\n",
        "    ```{text}```\n",
        "    '''\n",
        "    result = prompt_model(prompt, model=model)\n",
        "    pov = prompt_response_to_pov(result)\n",
        "    return pov\n",
        "\n",
        "def remove_leading_and_trailing_fluff(text):\n",
        "    model = 'gpt-4-turbo-preview'\n",
        "    prompt = f'''\n",
        "\n",
        "    You are a professional editor. You will be given a summary of a chapter from some book.\n",
        "\n",
        "    * Remove the leading and trailing backticks (```) from the text if they exist.\n",
        "    * Remove any leading or trailing whitespace from the text.\n",
        "    * Remove any leading title or leading chapter name from the text.\n",
        "\n",
        "    ```{text}```\n",
        "\n",
        "    YOUR EDITED TEXT:\n",
        "    '''\n",
        "\n",
        "    result = prompt_model(prompt, model=model)\n",
        "    return result\n",
        "\n",
        "def rewrite_in_pov(text, pov):\n",
        "    model = 'gpt-4-1106-preview'\n",
        "    prompt = f'''\n",
        "\n",
        "    You are a professional editor. You will be given a summary of a chapter from some book.\n",
        "\n",
        "    * Rewrite the summary in the point of view of {pov}.\n",
        "\n",
        "    ```{text}```\n",
        "\n",
        "    YOUR REWRITTEN TEXT:\n",
        "    '''\n",
        "\n",
        "    result = prompt_model(prompt, model=model)\n",
        "    return result\n",
        "\n",
        "def key_takeaway(text):\n",
        "    model = 'gpt-4-1106-preview'\n",
        "    prompt = f'''\n",
        "    You are a professional writer. You will be given a chapter from a book.\n",
        "    Your job is to identify the main idea of the text and summarize it in a few sentences.\n",
        "\n",
        "    * The main idea should have a title.\n",
        "    * The main idea should be a few sentences long.\n",
        "    * The main idea should follow the format [Title]: [Main Idea]\n",
        "\n",
        "    ```{text}```\n",
        "\n",
        "    Here is an example of the format:\n",
        "\n",
        "    ```The Allure of \"FREE!\": The chapter examines the psychological impact of the concept of \"free\" on consumer behavior, demonstrating through experiments and anecdotes how people are irrationally drawn to free products or services, even when they are not the best option. Despite only a trivial price difference, the presence of a free option can significantly shift consumers' preferences. This effect is so strong that it can influence not only purchasing decisions but also the success of public policies and social programs, highlighting the unique and powerful role of \"zero price\" in human psychology and decision-making.```\n",
        "\n",
        "    MAIN IDEA:\n",
        "    '''\n",
        "\n",
        "    result = prompt_model(prompt, model=model)\n",
        "    return result\n",
        "\n",
        "def key_takeaway_and_clean(chapter):\n",
        "    summary = key_takeaway(chapter[\"contents\"])\n",
        "    cleaned_summary = remove_leading_and_trailing_fluff(summary)\n",
        "    return cleaned_summary\n",
        "\n",
        "def process_chapter(chapter):\n",
        "    # Try to process a single chapter and handle possible exceptions\n",
        "    try:\n",
        "        summary = key_takeaway_and_clean(chapter)\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing chapter: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_key_takeaways_summary(data):\n",
        "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
        "        # Submit all chapter processing jobs to the executor\n",
        "        futures = [executor.submit(process_chapter, chapter) for chapter in data]\n",
        "        # Collect results as they become available\n",
        "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
        "            result = future.result()\n",
        "            if result is not None:\n",
        "                data[i][\"key_takeaway\"] = result\n",
        "            else:\n",
        "                data[i][\"key_takeaway\"] = \"Error processing this chapter\"\n",
        "    return data\n",
        "\n",
        "# Call the function to process the data\n",
        "processed_data = generate_key_takeaways_summary(data)\n",
        "\n",
        "def count_chapter_lengths(data):\n",
        "    for chapter in data:\n",
        "        len_words = get_token_count(chapter['contents'])\n",
        "        print(f\"Chapter {chapter['name']} has {len_words} tokens.\")\n",
        "\n",
        "count_chapter_lengths(processed_data)\n",
        "\n",
        "# Save the processed data to a file and offer it for the user to download\n",
        "output_filename = f\"{isbn}_key_takeaways.json\"\n",
        "with open(output_filename, \"w\") as output_file:\n",
        "    json.dump(processed_data, output_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "files.download(output_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}